{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f21f045a-88c7-4016-98b4-b8a3322dc8be",
   "metadata": {},
   "source": [
    "# Dataset Mashup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da6d8b5-c6cd-4fe9-9d52-954308ee1c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dafef7b-57c2-4e9e-a506-e33353dc23ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing files\n",
    "# D1_1 -> Total mortality in prison\n",
    "df_D1_1 = pd.read_csv('processed_data/D1.1.csv',sep=';')\n",
    "# D1_2x -> Death with reason 1/2/3/4\n",
    "df_D1_21 = pd.read_csv('processed_data/D1.21.csv',sep=';')\n",
    "df_D1_22 = pd.read_csv('processed_data/D1.22.csv',sep=';')\n",
    "df_D1_23 = pd.read_csv('processed_data/D1.23.csv',sep=';')\n",
    "df_D1_24 = pd.read_csv('processed_data/D1.24.csv',sep=';')\n",
    "\n",
    "# D2_1x -> People held in prison total/Male/Female\n",
    "df_D2_10 = pd.read_csv('processed_data/D2.10.csv',sep=';')  #D2 is not structured correctly\n",
    "df_D2_11 = pd.read_csv('processed_data/D2.11.csv',sep=';')\n",
    "df_D2_12 = pd.read_csv('processed_data/D2.12.csv',sep=';')\n",
    "# D2_2x -> People held unsentenced with timespan 1/2/3/4\n",
    "df_D2_21 = pd.read_csv('processed_data/D2.21.csv',sep=';')\n",
    "df_D2_22 = pd.read_csv('processed_data/D2.22.csv',sep=';')\n",
    "df_D2_23 = pd.read_csv('processed_data/D2.23.csv',sep=';')\n",
    "df_D2_24 = pd.read_csv('processed_data/D2.24.csv',sep=';')\n",
    "\n",
    "# D3 -> Official prison capacity per 100 thousand inhabitants and the actual population of the prison\n",
    "df_D3 = pd.read_csv('processed_data/D3.csv',sep=';')\n",
    "\n",
    "# D4 -> Percieved perception of the justice system \n",
    "df_D4 = pd.read_csv('processed_data/D4.csv',sep=';')\n",
    "\n",
    "# D5 -> Corruption perception index (unit scale 0 - 100 where 0 being `highly corrupt`)\n",
    "df_D5 = pd.read_csv('processed_data/D5.csv',sep=';')\n",
    "\n",
    "# D7 ->  General Government Sector's annual expenditure on \n",
    "#        [GF03] Public order and safety \n",
    "#        [GF0301] Police services \n",
    "#        [GF0303] Law courts \n",
    "#        [GF0304] Prisons\n",
    "#        [GF0305] R&D Public order and safety \n",
    "#        [GF0306] Public order and safety n.e.c.\n",
    "df_D7 = pd.read_csv('processed_data/D7.csv',sep=';')\n",
    "\n",
    "df_D1_23.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60fa6e6-fb46-4e62-8382-5f2ce74421ae",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Helper Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e653e2-e003-4193-88b0-3772d3ff4421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining some handy functions\n",
    "def col_unique_val(df,colname):\n",
    "    uniq_vals = []\n",
    "    for index,row in df.iterrows():\n",
    "        for col,value in row.items():\n",
    "            if col == colname and value not in uniq_vals:\n",
    "                uniq_vals.append(value)\n",
    "    return uniq_vals\n",
    "\n",
    "def row_values(df,row_index):\n",
    "    values = df.loc[row_index].values\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8914628-3e82-461f-b3eb-edcd4cd2487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the function to drop select columns from a df\n",
    "def col_drop(df):\n",
    "    years = ['2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012','2013','2014','2015','2021','2022']\n",
    "    to_keep = [col for col in df.columns if col.strip() not in years] #we use the strip to remove blank spaces around the value.\n",
    "    df = df[to_keep]\n",
    "    return df\n",
    "\n",
    "#function to remove space characters from your strings (useful to make correct string matches)\n",
    "def spaceRemover(df):\n",
    "    for column_name in df.columns:\n",
    "        if ' ' in column_name:\n",
    "            output_str = column_name.strip().replace(\" \", \"\")\n",
    "            df = df.rename(columns={column_name:output_str})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99960697-f271-4ef5-9704-47e9317bb53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to replace values of a column in a df using a dictionary for new values\n",
    "def replaceValue(df,col,mapping_dict):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].apply(lambda x: mapping_dict.get(x, x))\n",
    "    return df\n",
    "\n",
    "#the function to remove unwanted countries from the dataset.\n",
    "def countryRem(df,cntryList,col):\n",
    "    dfgrp = df.groupby(col)\n",
    "    resultdf = pd.DataFrame()\n",
    "    for cntry in dfgrp.groups:\n",
    "        if cntry in cntryList:\n",
    "            dftemp = dfgrp.get_group(cntry)\n",
    "            resultdf = pd.concat([resultdf,dftemp])\n",
    "        \n",
    "    resultdf = resultdf.reset_index(drop=True)\n",
    "    return resultdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4482dc50-efe2-4236-b17f-22a85623e249",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr>\n",
    "\n",
    "## Data Filtering and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5dea48-1085-42f7-b9f3-bc9bfc17cf92",
   "metadata": {},
   "source": [
    "At this stage, we set the timespan we are going to be working with ->  `2016 - 2021` <br>\n",
    "We also define the countries we are interested in for each sub-region of europe,for sake of simplicity, lets pick 5 countries from each sub-region. Please note that these allocation of any country to a subregions is `purely on the basis of their geography`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f36c1df-0d3e-41c3-901b-d83b94fb4060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#droping the non-required years for all the dfs\n",
    "df_D1_1 = col_drop(df_D1_1)\n",
    "df_D1_21 = col_drop(df_D1_21)\n",
    "df_D1_22 = col_drop(df_D1_22)\n",
    "df_D1_23 = col_drop(df_D1_23)\n",
    "df_D1_24 = col_drop(df_D1_24)\n",
    "\n",
    "df_D2_10 = col_drop(df_D2_10)\n",
    "df_D2_11 = col_drop(df_D2_11)\n",
    "df_D2_12 = col_drop(df_D2_12)\n",
    "df_D2_21 = col_drop(df_D2_21)\n",
    "df_D2_22 = col_drop(df_D2_22)\n",
    "df_D2_23 = col_drop(df_D2_23)\n",
    "df_D2_24 = col_drop(df_D2_24)\n",
    "\n",
    "df_D3 = col_drop(df_D3)\n",
    "df_D4 = col_drop(df_D4)\n",
    "df_D5 = col_drop(df_D5)\n",
    "df_D7 = col_drop(df_D7)\n",
    "\n",
    "df_D2_21.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ec8f40-aa4f-4ff5-a9bc-7a879b89af7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Let's reduce the number of countries we are working with. Here is the list of 5 countries from each region of europe which we have choosen to include in our project.**\n",
    "\n",
    "> **North_E   &#8594;** The selected countries have data in all the datasets<br>\n",
    "    - Estonia <br>\n",
    "    - Sweden <br>\n",
    "    - Finland <br>\n",
    "    - Denmark <br>\n",
    "    - Ireland <br>\n",
    "    \n",
    "> **East_E   &#8594;** The selected countries have data in all the datasets<br>\n",
    "    - Bulgaria <br>\n",
    "    - Hungary <br>\n",
    "    - Poland <br>\n",
    "    - Romania <br>\n",
    "    - Slovakia <br>\n",
    "    \n",
    "> **South_E   &#8594;** The selected countries have data in all the datasets<br>\n",
    "    - Slovenia <br>\n",
    "    - Greece <br>\n",
    "    - Italy <br>\n",
    "    - Portugal <br>\n",
    "    - Spain <br>\n",
    "    \n",
    "> **West_E   &#8594;**  The selected countries have data in all the datasets<br>\n",
    "    - Austria <br>\n",
    "    - France <br> \n",
    "    - Germany <br>\n",
    "    - Netherlands <br>\n",
    "    - Belgium <br>\n",
    "    \n",
    "### We will now normalise the code of the country in our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329f13d1-3e29-43f4-936c-f0b938107521",
   "metadata": {},
   "outputs": [],
   "source": [
    "countrygrp = df_D4.groupby(['geo'])\n",
    "onecountrydf = countrygrp.get_group('FR')\n",
    "onecountrydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02830d0-1c6a-4b2a-86c0-cb8b82a68c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "countryCode = {'AT':'Austria','BE':'Belgium','BG':'Bulgaria','CY':'Cyprus','CZ':'Czechia','DE':'Germany','DK':'Denmark','EE':'Estonia','EL':'Greece',\n",
    "               'ES':'Spain','FI':'Finland','FR':'France','HR':'Croatia','HU':'Hungary','IE':'Ireland','IT':'Italy','LT':'Lithuania','LU':'Luxembourg',\n",
    "               'LV':'Latvia','MT':'Malta','NL':'Netherlands','PL':'Poland','PT':'Portugal','RO':'Romania','SE':'Sweden','SI':'Slovenia','SK':'Slovakia',\n",
    "               'UK':'United Kingdom'}\n",
    "\n",
    "slctCountry = ['Estonia','Sweden','Finland','Denmark','Ireland',\n",
    "               'Bulgaria','Hungary','Poland','Romania','Slovakia',\n",
    "               'Slovenia','Greece','Italy','Portugal','Spain',\n",
    "               'Austria','France','Germany','Netherlands','Belgium']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b99363-0178-4be6-a9b7-8a91130c2043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#before replacement, we need to drop the EU27 and EU28\n",
    "# Delete rows with 'EU27_2020' and `EU28` in 'geo' column\n",
    "df_D4 = df_D4[df_D4['geo'] != 'EU27_2020']\n",
    "df_D4 = df_D4[df_D4['geo'] != 'EU28']\n",
    "df_D3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35e230a-6990-4b66-bbcb-a3fbcd24a53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing the name of countries in dfs to have same name in all datasets\n",
    "df_D3 = replaceValue(df_D3,'geo',countryCode)\n",
    "df_D4 = replaceValue(df_D4,'geo',countryCode)\n",
    "df_D5 = replaceValue(df_D5,'geo',countryCode)\n",
    "df_D7 = replaceValue(df_D7,'geo',countryCode)\n",
    "\n",
    "df_D3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16536caa-375f-48e9-b94c-075563bc4f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the unwanted countries from the datasets\n",
    "df_D1_1 = countryRem(df_D1_1,slctCountry,'Country')\n",
    "dfte11 = df_D1_1.groupby(['Country'])\n",
    "\n",
    "df_D1_21 = countryRem(df_D1_21,slctCountry,'Country')\n",
    "dfte121 = df_D1_21.groupby(['Country'])   #gets 13 countries  !!!CHECK SPELLINGS, but we get everything in the d1 total?\n",
    "df_D1_22 = countryRem(df_D1_22,slctCountry,'Country')\n",
    "dfte122 = df_D1_22.groupby(['Country'])  #gets 17 countries\n",
    "df_D1_23 = countryRem(df_D1_23,slctCountry,'Country')\n",
    "dfte123 = df_D1_23.groupby(['Country'])  #gets 16 countries \n",
    "df_D1_24 = countryRem(df_D1_24,slctCountry,'Country')\n",
    "dfte124 = df_D1_24.groupby(['Country'])  #gets 16 countries \n",
    "\n",
    "df_D2_10 = countryRem(df_D2_10,slctCountry,'Country')\n",
    "dfte210 = df_D2_10.groupby(['Country']) #gets 20 countries because of badly strcutured df\n",
    "df_D2_11 = countryRem(df_D2_11,slctCountry,'Country')\n",
    "dfte212 = df_D2_12.groupby(['Country']) #gets 20 countries because of badly strcutured df\n",
    "df_D2_12 = countryRem(df_D2_12,slctCountry,'Country')\n",
    "dfte212 = df_D2_12.groupby(['Country']) #gets 20 countries because of badly strcutured df\n",
    "df_D2_21 = countryRem(df_D2_21,slctCountry,'Country')\n",
    "dfte221 = df_D2_21.groupby(['Country']) #gets 8 countries \n",
    "df_D2_22 = countryRem(df_D2_22,slctCountry,'Country')\n",
    "dfte222 = df_D2_22.groupby(['Country']) #gets 8 countries \n",
    "df_D2_23 = countryRem(df_D2_23,slctCountry,'Country')\n",
    "dfte223 = df_D2_23.groupby(['Country']) #gets 8 countries \n",
    "\n",
    "\n",
    "df_D3 = countryRem(df_D3,slctCountry,'geo')\n",
    "df_D3 = df_D3.rename(columns={'geo': 'Country'})\n",
    "dfte3 = df_D3.groupby(['Country'])\n",
    "\n",
    "df_D4 = countryRem(df_D4,slctCountry,'geo')\n",
    "df_D4 = df_D4.rename(columns={'geo': 'Country'})\n",
    "dfte4 = df_D4.groupby(['Country'])\n",
    "\n",
    "df_D5 = countryRem(df_D5,slctCountry,'geo')\n",
    "df_D5 = df_D5.rename(columns={'geo': 'Country'})\n",
    "dfte5 = df_D5.groupby(['Country'])\n",
    "\n",
    "df_D7 = countryRem(df_D7,slctCountry,'geo')\n",
    "df_D7 = df_D7.rename(columns={'geo': 'Country'})\n",
    "dfte7 = df_D7.groupby(['Country'])\n",
    "\n",
    "#loop for counting the number of countries in the resulting df after filter\n",
    "count =  0\n",
    "for item in dfte221.groups:\n",
    "    count +=1 \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93734207-db47-4abd-92fe-0de24d215140",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now sub-dividing the dataframes with multiple categories of values.\n",
    "# FOR D3\n",
    "D3grp = df_D3.groupby('indic_cr')\n",
    "    \n",
    "    #PRIS_ACT_CAP\n",
    "    #PRIS_OFF_CAP\n",
    "df_D3_1 = D3grp.get_group('PRIS_OFF_CAP')\n",
    "df_D3_2 = D3grp.get_group('PRIS_ACT_CAP')\n",
    "\n",
    "df_D3_1 = countryRem(df_D3_1,slctCountry,'Country')\n",
    "df_D3_2 = countryRem(df_D3_2,slctCountry,'Country')\n",
    "    #now renaming the indic_cr to Category\n",
    "df_D3_1 = df_D3_1.rename(columns={'indic_cr': 'Category'})\n",
    "df_D3_2 = df_D3_2.rename(columns={'indic_cr': 'Category'})\n",
    "df_D3_2\n",
    "\n",
    "# FOR D4\n",
    "D4grp = df_D4.groupby('lev_per')\n",
    "\n",
    "    #FBAD\n",
    "    #FGOOD\n",
    "    #UNK\n",
    "    #VBAD\n",
    "    #VB_FB\n",
    "    #VGOOD\n",
    "    #VG_FG\n",
    "df_D4_1 = D4grp.get_group('FBAD')\n",
    "df_D4_2 = D4grp.get_group('FGOOD')\n",
    "df_D4_3 = D4grp.get_group('UNK')\n",
    "df_D4_4 = D4grp.get_group('VBAD')\n",
    "df_D4_5 = D4grp.get_group('VGOOD')\n",
    "\n",
    "df_D4_1 = df_D4_1.rename(columns={'lev_per':'Category'})\n",
    "df_D4_2 = df_D4_2.rename(columns={'lev_per':'Category'})\n",
    "df_D4_3 = df_D4_3.rename(columns={'lev_per':'Category'})\n",
    "df_D4_4 = df_D4_4.rename(columns={'lev_per':'Category'})\n",
    "df_D4_5 = df_D4_5.rename(columns={'lev_per':'Category'})\n",
    "\n",
    "# FOR D5\n",
    "# D5 contains only 1 category of values which is NR which is the corruption index of country , we will just rename the column it has values in as `Category`\n",
    "\n",
    "df_D5 = df_D5.rename(columns={'unit':'Category'})\n",
    "\n",
    "# FOR D7\n",
    "D7grp = df_D7.groupby('cofog99')\n",
    "\n",
    "    #GF03\n",
    "    #GF0301\n",
    "    #GF0303\n",
    "    #GF0304\n",
    "    #GF0305\n",
    "    #GF0306\n",
    "    \n",
    "df_D7_1 = D7grp.get_group('GF03')\n",
    "df_D7_2 = D7grp.get_group('GF0301')\n",
    "df_D7_3 = D7grp.get_group('GF0303')\n",
    "df_D7_4 = D7grp.get_group('GF0304')\n",
    "df_D7_5 = D7grp.get_group('GF0305')\n",
    "df_D7_6 = D7grp.get_group('GF0306')\n",
    "\n",
    "    #now renaming the `cofog99` to `Category`\n",
    "df_D7_1 = df_D7_1.rename(columns={'cofog99':'Category'}) \n",
    "df_D7_2 = df_D7_2.rename(columns={'cofog99':'Category'})  \n",
    "df_D7_3 = df_D7_3.rename(columns={'cofog99':'Category'})  \n",
    "df_D7_4 = df_D7_4.rename(columns={'cofog99':'Category'})  \n",
    "df_D7_5 = df_D7_5.rename(columns={'cofog99':'Category'})  \n",
    "df_D7_6 = df_D7_6.rename(columns={'cofog99':'Category'})  \n",
    "\n",
    "   #now renaming the `Indicator` to `Category` for D2 dataframes\n",
    "    \n",
    "df_D2_10 = df_D2_10.rename(columns={'Indicator':'Category'})\n",
    "df_D2_11 = df_D2_11.rename(columns={'Indicator':'Category'})\n",
    "df_D2_12 = df_D2_12.rename(columns={'Indicator':'Category'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93978d50-f8d2-421c-8a05-d4977955be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing values of a value in column\n",
    "def replace_value(df, col_name, value_str, replacement_str):\n",
    "    df[col_name] = df[col_name].replace(value_str, replacement_str)\n",
    "    return df\n",
    "\n",
    "df_D1_1 = replace_value(df_D1_1,'Category','Total','Total deaths')\n",
    "\n",
    "df_D2_10 = replace_value(df_D2_10,'Category','Persons held','Total persons held')\n",
    "df_D2_11 = replace_value(df_D2_11,'Category','Persons held','Male persons held')\n",
    "df_D2_12 = replace_value(df_D2_12,'Category','Persons held','Female persons held')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74b77da-3cad-4c65-993d-f496aee7f2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_D1_1.head(2)\n",
    "#print(df_D4_1.loc[1,'Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a125027a-4986-4dc2-9613-b73a07b8a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_unique_val(df_D7_1,'unit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bc19ee-04c8-41a0-b9f0-add4a4a220ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if there are space characters in the dfs\n",
    "for item in df_D3_1.columns:\n",
    "    print(item)\n",
    "    if ' ' in item:\n",
    "        print(True)\n",
    "        \n",
    "        print(type(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f43ddd-8e5b-4781-8ef4-1823b7603b79",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Final Dataset description\n",
    "\n",
    "After cleaning and filtering the source data according to the needs of the project,the results can be fetched and mashed together using the helper functions designed to generate the custom dataframe/s required for specific analysis between different variables.\n",
    "\n",
    "Given below are the names of different columns in different datasets and what kind of values and variable they hold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0981322b-8c23-4712-908b-d412adf0a949",
   "metadata": {},
   "source": [
    "> What are the values held in D1 datasets?\n",
    "\n",
    "| DF Name | variable| value type |\n",
    "|-------------|----------|----------|\n",
    "|   df_D1_1   | Total deaths                                   |   int - count   |\n",
    "|   df_D1_21  | Deaths due to external causes: by accident or other causes  |   int - count   |\n",
    "|   df_D1_22  | Deaths due to external causes: by intentional homicide      |   int - count   |\n",
    "|   df_D1_23  | Deaths due to external causes: by suicide                   |   int - count   |\n",
    "|   df_D1_24  | Deaths due to natural causes                                |   int - count   |\n",
    "\n",
    "> What are the values held in D2 datasets?\n",
    "\n",
    "| DF Name | variable | value type |\n",
    "|-------------|----------|----------|\n",
    "|   df_D2_10  | Total persons held   |   int - count    |\n",
    "|   df_D2_11  | Male persons held    |   int - count    |\n",
    "|   df_D2_12  | Female persons held  |   int - count    |\n",
    "|   df_D2_21  | Unsentenced for less than 12 months              |   int - count    |\n",
    "|   df_D2_22  | Unsentenced for less than 6 months               |   int - count    |\n",
    "|   df_D2_23  | Unsentenced for more than 12 months              |   int - count    |\n",
    "|   df_D2_24  | Unsentenced: Total                               |   int - count    |\n",
    "\n",
    "> What are the values held in D3 datasets?\n",
    "\n",
    "| DF Name | variable | value type |\n",
    "|-------------|----------|----------|\n",
    "|   df_D3_1  | PRIS_OFF_CAP   |   int - count/100,000 inhabitants    |\n",
    "|   df_D3_2  | PRIS_ACT_CAP   |   int - count/100,000 inhabitants    |\n",
    "\n",
    "> What are the values held in D4 datasets? <br>\n",
    "This dataset gives the perceptoion of people towards the justice system in percentage\n",
    "\n",
    "| DF Name | variable | value type |\n",
    "|-------------|----------|----------|\n",
    "|   df_D4_1  | FBAD   |   int - percentage   |\n",
    "|   df_D4_2  | FGOOD  |   int - percentage   |\n",
    "|   df_D4_3  | UNK    |   int - percentage   |\n",
    "|   df_D4_4  | VBAD   |   int - percentage   |\n",
    "|   df_D4_5  | VGOOD  |   int - percentage   |\n",
    "\n",
    "> What are the values held in D5 datasets?<br>\n",
    "<span style='color:green'>the name NR stands for national ranking in terms of corruption of the justice system.</span>\n",
    "\n",
    "| DF Name | variable | value type |\n",
    "|-------------|----------|----------|\n",
    "|   df_D5    | NR  |   int - percentage   |\n",
    "\n",
    ">  GEO <br> VALUE/YEAR <br> <span style='color:green'>_The indicator(value) is a composite index based on a combination of surveys and assessments of corruption from 13 different sources and scores and ranks countries based on how corrupt a countryâ€™s public sector is perceived to be, with a score of 0 representing a very high level of corruption and a score of 100 representing a very clean country._ </span>\n",
    "\n",
    "<br>\n",
    "\n",
    "**surprise surprise ! There is no D6** \n",
    "\n",
    "> What are the values held in D7 datasets? <br>\n",
    "<span style='color:green'>This dataset provided the goverment expenditure in different areas. The expenditure values can be acceesed using the expenditure type code provided in the table. To know the meaning of the code, see the documention Notebook.</span>\n",
    "\n",
    "| DF Name | variable | value type |\n",
    "|-------------|----------|----------|\n",
    "|   df_D7_1  | GF03     |   int - percentage of GDP   |\n",
    "|   df_D7_2  | GF0301   |   int - percentage of GDP   |\n",
    "|   df_D7_3  | GF0303   |   int - percentage of GDP   |\n",
    "|   df_D7_4  | GF0304   |   int - percentage of GDP   |\n",
    "|   df_D7_5  | GF0305   |   int - percentage of GDP   |\n",
    "|   df_D7_6  | GF0306   |   int - percentage of GDP   |\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6d18de-a541-4536-9e2e-83c7d7398a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the function for the making the multilevel index\n",
    "def midx_creator(country_name,country_datatype):\n",
    "    # country_name should be a string\n",
    "    # country_datatype should be a list of strings\n",
    "    code1 = []\n",
    "    code2 = []\n",
    "    for i in range(len(country_datatype)):\n",
    "        code1.append(0)\n",
    "        code2.append(i)\n",
    "        \n",
    "    midx = pd.MultiIndex(levels=[[country_name],\n",
    "                                  country_datatype],\n",
    "                         codes = [code1,\n",
    "                                  code2])\n",
    "    return midx\n",
    "\n",
    "def data_creator(df_list,country_name,country_datatype):\n",
    "    # WIP\n",
    "    # df_list needs to be a list\n",
    "    data = []\n",
    "    columns = ['2016','2017','2018','2019','2020']\n",
    "    return_datatype = []\n",
    "    for df in df_list:\n",
    "        df = df.reset_index(drop=True)\n",
    "        #print(df.head(1))\n",
    "        for dtype in country_datatype:\n",
    "            if dtype  == df.loc[1, 'Category']:  # the actual string value we are interested in \n",
    "                # IMPORTANT !!! we can create the country_datatype list in this loop so it will always be \n",
    "                # the same order as the list of list of datas\n",
    "                # BUT! if the country doesnt exist in the df? we need to now check if the country has data\n",
    "                # in the current df.\n",
    "                if country_name in df['Country'].values:\n",
    "                    return_datatype.append(dtype)\n",
    "                \n",
    "                    # select specific columns from the row where country is available\n",
    "                    row_values = df.loc[df['Country'] == country_name, columns].values.tolist()[0]\n",
    "                    data.append(row_values) \n",
    "            else:\n",
    "                continue\n",
    "                    \n",
    "    new_df = pd.DataFrame(index=midx_creator(country_name,return_datatype),\n",
    "                          columns=['2016','2017','2018','2019','2020'],\n",
    "                          data = data)\n",
    "    #print(return_datatype)\n",
    "    #print(row_values)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d1e950-b063-44f5-b124-49b99c1b1199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing a function to take in multiple countries as input and the list of categories we are interested in from the user.\n",
    "\n",
    "def masher(countries,categories,df_list):\n",
    "    #countries need to be a listitem\n",
    "    resultDF = pd.DataFrame()\n",
    "    for countryName in countries:\n",
    "        tempDF = data_creator(df_list,countryName,categories)\n",
    "        resultDF = pd.concat([resultDF,tempDF])\n",
    "    \n",
    "    return resultDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e610c0-37c8-42a1-91a6-316f3d0fd509",
   "metadata": {},
   "source": [
    "## <span style='color:silver;'>Part I</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9b9015-664e-4947-bd56-e0c5026d9950",
   "metadata": {},
   "source": [
    "We created the dataframe for individual countries which we plan on analysing which includes all the different types of data we have from different sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b80051-3287-437d-a27d-4cc2c9dbdc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the df_list\n",
    "df_D3_1 = spaceRemover(df_D3_1)\n",
    "df_D3_2 = spaceRemover(df_D3_2)\n",
    "\n",
    "df_D4_1 = spaceRemover(df_D4_1)\n",
    "df_D4_2 = spaceRemover(df_D4_2)\n",
    "df_D4_3 = spaceRemover(df_D4_3)\n",
    "df_D4_4 = spaceRemover(df_D4_4)\n",
    "df_D4_5 = spaceRemover(df_D4_5)\n",
    "\n",
    "df_D5 = spaceRemover(df_D5)\n",
    "\n",
    "df_D7_1 = spaceRemover(df_D7_1)\n",
    "df_D7_2 = spaceRemover(df_D7_2)\n",
    "df_D7_3 = spaceRemover(df_D7_3)\n",
    "df_D7_4 = spaceRemover(df_D7_4)\n",
    "df_D7_5 = spaceRemover(df_D7_5)\n",
    "df_D7_6 = spaceRemover(df_D7_6)\n",
    "\n",
    "dfList = [df_D1_1,\n",
    "          df_D1_21,\n",
    "          df_D1_22,\n",
    "          df_D1_23,\n",
    "          df_D1_24,\n",
    "          df_D2_10,\n",
    "          df_D2_11,\n",
    "          df_D2_12,\n",
    "          df_D2_21,\n",
    "          df_D2_22,\n",
    "          df_D2_23,\n",
    "          df_D2_24,\n",
    "          df_D3_1,\n",
    "          df_D3_2,\n",
    "          df_D4_1,\n",
    "          df_D4_2,\n",
    "          df_D4_3,\n",
    "          df_D4_4,\n",
    "          df_D4_5,\n",
    "          df_D5,\n",
    "          df_D7_1,\n",
    "          df_D7_2,\n",
    "          df_D7_3,\n",
    "          df_D7_4,\n",
    "          df_D7_5,\n",
    "          df_D7_6,\n",
    "          ]\n",
    "dfD1clean=[df_D1_1,df_D1_21,df_D1_22,df_D1_23,df_D1_24]\n",
    "dfD2clean = [df_D2_10,df_D2_11,df_D2_12,df_D2_21,df_D2_22,df_D2_23,df_D2_24]\n",
    "dfD3clean = [df_D3_1,df_D3_2,]\n",
    "dfD4clean = [df_D4_1,df_D4_2]\n",
    "dfD5clean = [df_D5]\n",
    "dfD6clean = [df_D7_1,df_D7_2,df_D7_3,df_D7_4,df_D7_5,df_D7_6,]\n",
    "country_datatype = ['Total deaths',\n",
    "                    'Deaths due to external causes: by accident or other causes',\n",
    "                    'Deaths due to external causes: by intentional homicide',\n",
    "                    'Deaths due to external causes: by suicide',\n",
    "                    'Deaths due to natural causes',\n",
    "                    \n",
    "                    'Total persons held',\n",
    "                    'Male persons held',\n",
    "                    'Female persons held',\n",
    "                    'Unsentenced for less than 6 months',\n",
    "                    'Unsentenced for less than 12 months',\n",
    "                    'Unsentenced for more than 12 months',\n",
    "                    'Unsentenced: Total',\n",
    "                    \n",
    "                    'PRIS_OFF_CAP',\n",
    "                    'PRIS_ACT_CAP',\n",
    "                    \n",
    "                    'FBAD',\n",
    "                    'FGOOD',\n",
    "                    'UNK',\n",
    "                    'VBAD',\n",
    "                    'VGOOD',\n",
    "                    \n",
    "                    'NR',\n",
    "                    \n",
    "                    'GF03',\n",
    "                    'GF0301',\n",
    "                    'GF0303',\n",
    "                    'GF0304',\n",
    "                    'GF0305',\n",
    "                    'GF0306']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d15865-cf35-4a55-8ed1-cba1953cb8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_lst = ['Estonia','Sweden','Finland','Denmark','Ireland','Bulgaria','Hungary','Poland','Romania','Slovakia','Slovenia','Greece','Italy','Portugal','Spain','Austria','France','Germany','Netherlands','Belgium']\n",
    "df_1 = masher(country_lst,country_datatype,dfD1clean)\n",
    "df_2 = masher(country_lst,country_datatype,dfD2clean)\n",
    "df_3 = masher(country_lst,country_datatype,dfD3clean)\n",
    "df_4 = masher(country_lst,country_datatype,dfD4clean)\n",
    "df_5 = masher(country_lst,country_datatype,dfD5clean)\n",
    "df_6 = masher(country_lst,country_datatype,dfD6clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81245594-7a42-4a99-a169-47f2baaa3af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert datatype\n",
    "def convert_Dtype(Df):\n",
    "    Df = Df.astype(int)\n",
    "    \n",
    "    return Df\n",
    "\n",
    "# a senitnal value is used to replace all the ':' strings which denote missing values in our dataset. The choice of sentinal value is done based on the type\n",
    "# data we are working with and a negative value of 999 is distinct and non-feasable for our variable to achieve in realistic scenarious. \n",
    "sentinal_value = -999\n",
    "df_1.replace(\":\", sentinal_value, inplace=True)\n",
    "\n",
    "df_1 = convert_Dtype(df_1)\n",
    "print(df_1.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cb4ace-3ec3-4c92-a6b5-bc5d92766722",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.replace(\":\", sentinal_value, inplace=True)\n",
    "df_2 = convert_Dtype(df_2)\n",
    "print(df_1.dtypes)\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dd9ad2-7596-4233-8f7e-d8e9439059ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_space_chars(df):\n",
    "    \"\"\"\n",
    "    Check if any value in a dataframe contains a space character at the beginning or end of it.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataframe to check.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if any value in the dataframe contains a space character at the beginning or end of it, False otherwise.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        for value in df[col]:\n",
    "            if isinstance(value, str) and (value.startswith(' ') or value.endswith(' ')):\n",
    "                print(\"this is true\")\n",
    "                return True\n",
    "            \"print this is false\"\n",
    "    return False\n",
    "\n",
    "#check_space_chars(df_3)\n",
    "df_3_copy = df_3\n",
    "\n",
    "def remove_space_chars(df):\n",
    "    \"\"\"\n",
    "    Remove any space characters at the beginning or end of a value in a dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataframe to modify.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The modified dataframe with space characters removed.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].str.strip()\n",
    "    return df\n",
    "\n",
    "df_3_copy = remove_space_chars(df_3_copy)\n",
    "check_space_chars(df_3_copy)\n",
    "\n",
    "df_3.replace(np.nan, sentinal_value, inplace=True)\n",
    "\n",
    "#df_2 = convert_Dtype(df_2)\n",
    "#print(df_1.dtypes)\n",
    "df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28abc80-be86-4c94-a5a8-280c84aebc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4.replace(\":\", sentinal_value, inplace=True)\n",
    "df_4 = convert_Dtype(df_4)\n",
    "print(df_4.dtypes)\n",
    "df_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f180f1d-31e4-470c-91ce-6203002a9086",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5.replace(\":\", sentinal_value, inplace=True)\n",
    "df_5 = convert_Dtype(df_5)\n",
    "print(df_5.dtypes)\n",
    "df_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1099be4-6b95-4bc6-92e3-3cc8934715a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_6.dtypes)\n",
    "print(df_6)\n",
    "\n",
    "#df_6 = df_6.astype(float)\n",
    "#print(df_6.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cf3ae7-a1be-4e2f-a1c3-09e8409c9a9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style='color:silver;'>Part II</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6d660d-c0d0-4598-afd8-5718b51da8eb",
   "metadata": {},
   "source": [
    "We will now convert the mashup dataframe into a json file for to be used in the visualisation using `AmCharts.js` for deploying on the web."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb68b6c-a668-4b8e-a846-c72e7f15e3b3",
   "metadata": {},
   "source": [
    "## JSON converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbe3e83-3a7e-49a4-bb58-255de6a4e067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def df_to_json(dataframe):\n",
    "    \"\"\"\n",
    "    Convert a Pandas DataFrame to a JSON object.\n",
    "    \"\"\"\n",
    "    json_str = dataframe.to_json(orient='table')  # there are many possible orientations like `records`, `split` and `table` -  default = `index`\n",
    "    json_obj = json.loads(json_str)\n",
    "    return json_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c66853c-0227-49e0-ac3b-c26abce9e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the above function\n",
    "testJS = json.dumps(df_to_json(dfFINE), indent=4)\n",
    "print(testJS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9d2401-7fd3-4f99-9727-3a686e574014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporting the json file \n",
    "with open('data.json', 'a') as f:\n",
    "    f.write(json.dumps(testJS, ensure_ascii=False, indent=4,sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d8331-6176-4a0e-9675-3588f8468a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporting as CSV\n",
    "df_1.to_csv('D1_deaths_in_prison_clean.csv',sep=',',encoding='utf-8')\n",
    "df_2.to_csv('D2_sentance_status_clean.csv',sep=',',encoding='utf-8')\n",
    "df_3.to_csv('D3_prison_capacity_clean.csv',sep=',',encoding='utf-8')\n",
    "df_4.to_csv('D4_perceived_independence_of_the_justice_system_clean.csv',sep=',',encoding='utf-8')\n",
    "df_5.to_csv('D5_CPI_from_eurostat_clean.csv',sep=',',encoding='utf-8')\n",
    "df_6.to_csv('D6_general_governments_expenditure_clean.csv',sep=',',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cb4675-736c-40d9-9520-826687df06e1",
   "metadata": {},
   "source": [
    "## ROUGH WORK\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98faca85-dea6-4dfc-b5a5-7fc0aa008945",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a sample dataframe with a multi-index\n",
    "arrays = [['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'],\n",
    "          ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]\n",
    "tuples = list(zip(*arrays))\n",
    "index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])\n",
    "df = pd.DataFrame({'A': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "                   'B': [10, 20, 30, 40, 50, 60, 70, 80],\n",
    "                   'C': [100, 200, 300, 400, 500, 600, 700, 800]},\n",
    "                  index=index)\n",
    "\n",
    "# print the original dataframe\n",
    "print(df)\n",
    "\n",
    "# select data using levels\n",
    "'''\n",
    "print(df.loc[('bar', 'one')])  # select data for bar and one\n",
    "print(df.loc['foo'])  # select all data for foo\n",
    "print(df.loc[:, 'B'])  # select all data for B column\n",
    "print(df.loc[('baz', 'two'), 'C'])  # select data for baz, two and C column\n",
    "'''\n",
    "\n",
    "midx = pd.MultiIndex(levels=[['first level values','frist_level_2'],\n",
    "                            ['second level v1', 'second level v2']],\n",
    "                     codes=[[0,0,1,1],  #this is first level codes \n",
    "                           [0,1,0,1]])  #this is second level codes \n",
    "df = pd.DataFrame(index=midx,\n",
    "                  columns=['year1','year2','year3'],\n",
    "                  data = [[1,2,3],\n",
    "                          [4,5,6],\n",
    "                          [1,2,3],\n",
    "                          [4,5,6]])\n",
    "df.loc[('first level values')]\n",
    "df\n",
    "\n",
    "df.loc[('first level values','second level v2')]\n",
    "for col,value in df.loc[('first level values','second level v2')].items():\n",
    "    print(col + 'has value ' + str(value))\n",
    "\n",
    "# create a list of tuples for each level of the index\n",
    "index_levels = [\n",
    "    ['Group A', 'Group A', 'Group B', 'Group B'],\n",
    "    ['Feature 1', 'Feature 2', 'Feature 1', 'Feature 2']\n",
    "]\n",
    "\n",
    "# create a list of codes for each level of the index\n",
    "index_codes = [\n",
    "    [0, 1, 0, 1],\n",
    "    [0, 1, 0, 1]\n",
    "]\n",
    "\n",
    "# create the multi-level index using the levels and codes\n",
    "index = pd.MultiIndex.from_tuples(list(zip(*index_levels)), names=['Group', 'Feature'])\n",
    "index.codes = index_codes\n",
    "\n",
    "# create a dataframe with the multi-level index\n",
    "df = pd.DataFrame({'Values': [1, 2, 3, 4]}, index=index)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435a1182-b7b9-4025-8a59-55e0cc6833e5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# create a sample dataframe\n",
    "df = pd.DataFrame({'col1': [1, 2, 3], 'col2': ['a', 'b', 'c'], 'col3': [True, False, True]})\n",
    "print(df)\n",
    "\n",
    "# select specific columns from the row where col3 is True\n",
    "row_values = df.loc[df['col3'] == True, ['col1', 'col2']].values.tolist()[0]\n",
    "'''\n",
    "we get a new DataFrame that contains only the rows where the value of col3 is True. Since there may be more than one row that satisfies the condition, \n",
    "the result is still a DataFrame, not a single row. So we specify '[0]' to get only the first row\n",
    "'''\n",
    "\n",
    "# print the row values\n",
    "print(row_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940efdf5-a8b4-46ac-b4f5-789f9cf2ce80",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#prova\n",
    "midx = pd.MultiIndex(levels=[slctCountry,\n",
    "                            ['second level v1', 'second level v2','second level v3']],\n",
    "                     codes=[[0,0,0,1,1,1],\n",
    "                           [0,1,2,0,1,2]])\n",
    "df = pd.DataFrame(index=midx,\n",
    "                  columns=['2016','2017','2018','2019','2020'],\n",
    "                  data = [[1,2,3,4,5],\n",
    "                          [1,22,3,4,5],\n",
    "                          [1,23,3,4,5],\n",
    "                          [1,2,3,4,5],\n",
    "                          [1,22,3,4,5],\n",
    "                          [1,23,3,4,5]])\n",
    "\n",
    "midx2 = pd.MultiIndex(levels=[slctCountry,\n",
    "                            ['second level v1', 'second level v2','second level v3']],\n",
    "                     codes=[[0,0,0,1,1,1],\n",
    "                           [0,1,2,0,1,2]])\n",
    "df2 = pd.DataFrame(index=midx,\n",
    "                  columns=['2016','2017','2018','2019','2020'],\n",
    "                  data = [[1,2,3,4,5],\n",
    "                          [1,22,3,4,5],\n",
    "                          [1,23,3,4,5],\n",
    "                          [1,2,3,4,5],\n",
    "                          [1,22,3,4,5],\n",
    "                          [1,23,3,4,5]])\n",
    "\n",
    "dfmergeTest = pd.concat([df,df2])\n",
    "dfmergeTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd82064-bfb5-4857-97e2-511f3ec05cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will we merging the required values from the D7/D4 & D5 datasets.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
